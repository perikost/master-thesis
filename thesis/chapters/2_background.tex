\chapter{Background \& Related Work}\label{chapter:background}

\section{Ethereum}\label{sec:}
\section{Distributed File Systems}\label{sec:}
In peer-to-peer distributed file systems, participating nodes share their resources to store and access all kinds of content. Nowadays, IPFS and Swarm are two of the most promising distributed file systems. Both have implemented, evolved, and connected widely researched techniques found in the domain of distributed systems. They aim to be platforms where users can trust the content they receive, without trusting the peers that provide it.

Unlike centralized systems where data is located and accessed based on its physical location (i.e. server address), distributed file systems utilize content addressing. Content addressing \citep{trautwein_2022} refers to a mechanism where data is identified and retrieved based on its content, rather than its location. Some features inherent to content addressing are:

\begin{itemize}
    \item \textbf{Decentralization and performance}: Nodes on the network can locate and retrieve data from any other node in the network that has the data, supporting decentralization and reducing overall latency.
    \item \textbf{Immutability and Integrity}: Immutability is a fundamental characteristic of content addressing. Once data is stored and its content-based address is generated, typically using a cryptographic hash function, any interference with the data would change its address. This ensures that the data cannot be altered without detection, reinforcing data integrity.
    \item \textbf{Deduplication}: Within a node's storage space, data is identified by its content. Therefore, even if the same data is added multiple times only one copy will be retained, saving storage space and improving system efficiency.
\end{itemize}

Beyond the benefits derived from content addressing, the decentralized nature of distributed file systems like IPFS and Swarm offers several advantages over centralized solutions \citep{daniel_2022, ipfs_docs_1}. Firstly, they achieve data redundancy, ensuring that content is replicated across multiple nodes, often by means of data caching. This redundancy enhances fault tolerance, as usually, the loss of a single node does not result in data loss. Furthermore, having multiple copies of data distributed across the network, the likelihood of data being accessible increases.

Except for fault tolerance and availability, data caching plays a vital role in improving the overall retrieval performance of distributed file systems. This is apparent in systems like IPFS and Swarm, where recently accessed data is cached. This means that when a node requests a specific piece of data, if it is cached by a nearby node, it can be retrieved more quickly, as it doesn't need to traverse the entire network. As one might expect, popular content tends to be replicated extensively across the network, reducing its retrieval latency.

Another key feature of IPFS and Swarm is that they are resilient against censorship. As already mentioned, data in these platforms is addressed by its content and isn't stored in a single location or controlled by a single entity. Instead, it's distributed across multiple nodes which makes it extremely difficult for an individual party to censor information. Once data is uploaded and as long as at least one node on the network has a copy of the content, it can be accessed from any other node, promoting freedom in terms of access to information \citep{IPFS_team_2017}.

In summary, distributed file systems present significant advantages over traditional centralized storage systems. By employing content-based addressing, they enable deduplication and location independence while enforcing data integrity. Also, since they offer decentralized access to data and leverage data caching, they provide data redundancy, censorship-resistance, availability and efficient retrieval, even in the face of node failures or network disruptions.

% TODO: choose
\subsection{IPFS}\label{subsection:}
\paragraph{Caching}\label{par:}
Nodes on the IPFS network, have a built-in caching mechanism that allows them to automatically cache content they have recently accessed. When nodes fetch data, they always store a copy in their local cache.

While content is present in a node's local storage, it can be served to other nodes who might request it, reducing retrieval latency, e.g. fetching a cached block from a neighboring node eliminates the overhead of a network traversal. This becomes particularly effective when dealing with popular content that is cached by multiple nodes, as the likelihood that the content is retrieved from nearby sources increases.

Cached content remains available until it is garbage collected. Garbage collection in the field of computer science refers to a process that identifies and removes content that is no longer in use, based on various strategies \citep{gc_2023}. In the case of IPFS, the garbage collection strategy involves identifying and discarding all \emph{``unpinned''} blocks \citep{ipfs_docs_2}. \hl{Explicitly pinned content and files added to the Mutable File System (MFS) - implicitly pinned content - are protected against garbage collection.}

\subsection{Swarm}\label{subsection:}
\paragraph{Caching}\label{par:}
Swarm has adopted an opportunistic caching mechanism where forwarding nodes \hl{(nodes that pass chunks to other nodes)} cache chunks they receive and relay in case they are requested again \citep[p.~47]{tron_2020}. More specifically, nodes in Swarm cache:

\begin{itemize}
    \item \textbf{Downloaded chunks}: Nodes cache chunks that they explicitly download.
    \item \textbf{Uploaded chunks}: Nodes cache chunks that they explicitly upload.
    \item \textbf{Requested chunks}: When a retrieval request is made, the desired chunks are forwarded from node to node until they reach the recipient \hl{(requesting node)}. All intermediate nodes cache the chunks.
    \item \textbf{Push-synced chunks}: During the upload process chunks are forwarded until they reach their neighborhood of responsibility. All intermediate nodes cache the chunk.
    \item \textbf{Pull-synced chunks}: Pull syncing aims to maximize resource utilization by pushing chunks to nodes that have surplus storage, as well as sync chunks within a neighborhood when the topology changes. Nodes that relay the chunks cache them.
\end{itemize} 

Nodes in the Swarm network store and serve chunks of data but unlike IPFS, are rewarded for their contribution. They receive a reward every time they serve a chunk. The profitability of a chunk is proportional to its popularity; the more often a chunk is requested, the higher the reward. Accordingly, the garbage collection method used takes into account the profitability factor, which is reasonably determined by the age of the last request (Least Recently Used (LRU) strategy) \citep[p.~72]{tron_2020}. Thus non-popular chunks are more likely to be removed, creating an auto-scaled content distribution network \citep[p.~72]{tron_2020} where popular chunks become more widely spread and much faster to retrieve. This helps optimize the overall performance and efficiency of the network.

Finally, Swarm has also adopted the concept of pinning which protects content against garbage collection, forcing the node to keep it permanently in its local storage. In contrast to IPFS, local pinning is not enough to ensure that a chunk is generally retrievable by other nodes since the pinner might not be in the neighborhood of responsibility where the chunk is meant to be stored. In order to overcome this, global pinning and a recovery process are proposed where pinners re-upload the missing chunk and then serve it to the requester \citep[pp.~161--167]{tron_2020}.

In summary, the automatic caching we described occurs as a side effect of Swarm's \hl{data distribution model}, which \hl{pushes/spreads} data through the network during upload and download, enhancing low latency data retrieval, availability, and robustness against failures. Apparently, popular data which is requested frequently is favored by the caching mechanism. However, even data missing from their neighborhood of responsibility can be retrieved through the recovery process.

\subsection{IPFS \& Swarm Side-by-Side}\label{sec:}
% \subsection{Overview of IPFS and Swarm as decentralized storage solutions (IPFS vs Swarm)}\label{sec:}
In this section, we present a side-by-side comparison of IPFS and Swarm emphasizing mainly their differences. By clarifying their distinct architectural and design choices, we aim to offer \hl{comprehensive/broader} overview and a better understanding of how each system achieves data availability, content addressing, censorship resistance, etc.

Content is split into chunks before being uploaded to either of these platforms. A chunk is a piece of data that represents the basic unit of storage and retrieval in IPFS and Swarm. Each chunk is uniquely referenced using its cryptographic hash to achieve content addressing instead of the widely used location addressing. This allows for quick verification of received data and enables deduplication. In Swarm, a Binary Merkle Tree (BMT)  \citep{tron_2020} hash algorithm based on Keccak-256 is used to generate the identifiers of the chunks. Note that for encrypted content, the identifier is the concatenation of the chunk hash and the decryption key and therefore is represented in 64 bytes. The developers of IPFS approached this matter differently to provide more flexibility to end users. They introduced CID  \citep{multiformat}, a self-describing upgradable content-addressed identifier, which consists of the hash digest of the content and some metadata as a prefix. From the metadata, the CID version and its encoding, as well as the hashing function used and the contentâ€™s type can be determined.

In order to \emph{``group''} chunks of the same data together, both platforms implement some sort of tree data structure. The root hash of such trees refers to the data as a whole. In IPFS a Merkle Directed Acyclic Graph (DAG)  \citep{benet_2014}, a generalized construction of a Merkle Tree, is used. It allows nodes to have multiple parents, does not need to be balanced, and non-leaf nodes can contain data. Swarm once more embraced the use of Merkle Trees, in which intermediate nodes contain links to leaf nodes that hold the chunks. In the case of uploading folders, IPFS recursively constructs a Merkle DAG, whereas Swarm utilizes a Trie  \citep{tron_2020} data structure. Both these schemes allow for Unix-like path resolving.

Another fundamental difference between IPFS and Swarm lies in what content participating nodes in each network store. In the former, a node holds content depending on its interests, while in the latter a \emph{``cloud''} is formed with every node storing arbitrary chunks of data originating from other nodes. In both cases, nodes also cache content they access, until it is \emph{``garbage collected''}. This mechanism is crucial in ensuring that the distribution of popular content scales automatically and retrieval latency is decreased. Swarm takes this one step further by implementing an opportunistic cache model: intermediate nodes cache all chunks they forward.

Moreover, the subjects at hand approach data discoverability and retrieval differently. In IPFS a distributed hash table (DHT)  \citep{benet_2014} is used to map CIDs to seeders that can serve that content. In Swarm, a similar model called DISC  \citep{tron_2020} was developed, in which content (value) is directly stored under a corresponding identifier (key), reducing the retrieval process to one routing request. Besides that, in both platforms data are public by default. However, Swarm offers the choice of encrypting data before an upload. Finally, Swarm has a built-in incentive system based on Ethereum, while IPFS depends on a separate blockchain, Filecoin. 

\section{Related work - Purpose and significance of the thesis}\label{sec:}
There are tools for assisting developers to minimize gas consumption, but mostly, the focus is on the program structures rather than the data  \citep{nelaturu_2021, chen_2017, chen_2021}. Among them, we singled out Gasper  \citep{chen_2017}, a cost minimization tool that analyzes bytecode to determine costly patterns in SCs. Its creators used it to analyze approximately 4000 SCs and concluded that three of these patterns were found in the majority of them. This research was continued, and new patterns have been added  \citep{chen_2021}. There is also work on parametric cost bounds in the Ethereum blockchain \citep{albert_2021}.

Data storage cost and gas-saving methods are studied to some extent in the context of specific use cases  \citep{kurt_2020, delgado_2019, westerkamp_2020}. Gas consumption of SCs is examined in  \citep{grech_2020, signer_2018}, but none of them researches the related cost holistically by considering all possible ways of using Ethereum as a data store. In  \citep{consensys}, the use of events as an alternative data storage option was proposed. However, the novel idea of storing data in a transactionâ€™s payload is barely studied. Blockchi \citep{yankov_2018} is a tool that allows storing and retrieving JSON objects, while in  \citep{xie_2017} a storage model is proposed for storing data from IoT sensors in hexadecimal format. In the latter, transaction references are retained in an external database. In our work, apart from evaluating the transaction payload as a data store, we propose an extension to this approach, which we term as \emph{``Unused Function Parameters''}. In short, it is a means of storing data in a transaction's payload while exploiting Solidity's built-in ABI interface to allow handling all available data types.

A common architecture for managing large chunks of data is to use distributed file systems such as IPFS and Swarm for storage and record the resulting content identifiers in Ethereum. This approach has been studied in several applications, e.g., \citep{hao_j_2018, ren_2021}, but no comments were made on the performance of these platforms nor on the cost of recording the content identifiers. Ramesh et al.\cite{ramesh_2019}, exploited such a scheme to handle IoT data and also studied both local retrieval and upload performances of IPFS and Swarm. In \citep{shen_2019}, IPFS was evaluated for remote and local retrieval latency. In \citep{abdullah_2021} the performance of IPFS in a private network was compared to that of FTP. Furthermore, a recent study conducted by Ismail et al. \cite{aisyah_2022} reviewed several works centered around the performance of distributed file systems. In light of the available literature, it is apparent that the majority of attention is being directed towards IPFS, leading us to believe that Swarmâ€™s performance is an under-researched topic. On top of that, it's worth mentioning that research on Swarm's performance prior to three years ago should now be considered outdated, as a new client \citep{swarm_bee} and updates to the underlying protocols were introduced in 2020 \citep{tron_2020}. Our work is focused on local read-write performance of IPFS and Swarm, and the cost associated with storing the corresponding content identifiers in Ethereum.

In conclusion, even though the necessity of minimizing gas consumption is recognized in the literature, existing studies focus mainly on program structures or inefficient patterns. At the same time, efficient data management is an under-researched topic despite the fact that it is an essential part of DApps. In this work, through the research question we posed and the corresponding research we conducted, we intend to give a comprehensive perspective on efficient ways of handling data both on-chain and in hybrid architectures.

\subsection{Existing data management approaches in Ethereum-based dApps}\label{subsection:}
\subsection{Cost and retrieval performance of the aforesaid approaches}\label{subsection:}
\subsection{Cost and retrieval performance of the hybrid approaches}\label{subsection:}
\subsection{Purpose and significance of the thesis}\label{subsection:}

