\chapter{Background \& Related Work}\label{chapter:background}

\section{Ethereum}\label{sec:ethereum}
\section{Distributed File Systems}\label{sec:dfs}
In peer-to-peer distributed file systems, participating nodes share their resources to store and access all kinds of content. Nowadays, IPFS and Swarm are two of the most promising distributed file systems. Both have implemented, evolved, and connected widely researched techniques found in the domain of distributed systems. They aim to be platforms where users can trust the content they receive, without trusting the peers that provide it.

Unlike centralized systems where data is located and accessed based on its physical location (i.e. server address), distributed file systems utilize content addressing. Content addressing \citep{trautwein_2022} refers to a mechanism where data is identified and retrieved based on its content, rather than its location. Some features inherent to content addressing are:

\begin{itemize}
    \item \textbf{Decentralization and performance}: Nodes on the network can locate and retrieve data from any other node in the network that has the data, supporting decentralization and reducing overall latency.
    \item \textbf{Immutability and Integrity}: Immutability is a fundamental characteristic of content addressing. Once data is stored and its content-based address is generated, typically using a cryptographic hash function, any interference with the data would change its address. This ensures that the data cannot be altered without detection, reinforcing data integrity.
    \item \textbf{Deduplication}: Within a node's storage space, data is identified by its content. Therefore, even if the same data is added multiple times only one copy will be retained, saving storage space and improving system efficiency.
\end{itemize}

Beyond the benefits derived from content addressing, the decentralized nature of distributed file systems like IPFS and Swarm offers several advantages over centralized solutions \citep{daniel_2022, ipfs_docs_1}. Firstly, they achieve data redundancy, ensuring that content is replicated across multiple nodes, often by means of data caching. This redundancy enhances fault tolerance, as usually, the loss of a single node does not result in data loss. Furthermore, having multiple copies of data distributed across the network, the likelihood of data being accessible increases.

Except for fault tolerance and availability, data caching plays a vital role in improving the overall retrieval performance of distributed file systems. This is apparent in systems like IPFS and Swarm, where recently accessed data is cached. This means that when a node requests a specific piece of data, if it is cached by a nearby node, it can be retrieved more quickly, as it doesn't need to traverse the entire network. As one might expect, popular content tends to be replicated extensively across the network, reducing its retrieval latency.

Another key feature of IPFS and Swarm is that they are resilient against censorship. As already mentioned, data in these platforms is addressed by its content and isn't stored in a single location or controlled by a single entity. Instead, it's distributed across multiple nodes which makes it extremely difficult for an individual party to censor information. Once data is uploaded and as long as at least one node on the network has a copy of the content, it can be accessed from any other node, promoting freedom in terms of access to information \citep{IPFS_team_2017}.

In summary, distributed file systems present significant advantages over traditional centralized storage systems. By employing content-based addressing, they enable deduplication and location independence while enforcing data integrity. Also, since they offer decentralized access to data and leverage data caching, they provide data redundancy, censorship-resistance, availability and efficient retrieval, even in the face of node failures or network disruptions.

\subsection{IPFS}\label{sec:ipfs}
% TODO: data or content
The InterPlanetary File System (IPFS) is an open-source, peer-to-peer network that aims to \hl{revolutionize/redefine} data storage and delivery through its innovative content-addressable system \citep{benet_2014}. As an open, permissionless system, IPFS is accessible to anyone, allowing participants to host and download data from other peers. 

Data, when uploaded, is split into chunks and is stored by a small set of nodes - the initial uploader and any nodes that download and cache the data - known as \textit{providers} who make the data available to other peers. Due to its decentralized architecture, IPFS ensures there's no single point of failure and operates effectively without the need for mutual trust between nodes.

A fundamental component of IPFS is the Kademlia based Distributed Hash Table (DHT) \citep{maymounkov_2002} that serves as a decentralized catalog and navigation system, routing content requests to data providers in the network. Essentially, it maintains mappings of data identifiers to peers and peers to their network addresses. Kademlia is known \citep{benet_2014} for its efficient lookup through massive networks and thereby facilitates quick content discovery and peer communication. The actual peer-to-peer connectivity as well as most of IPFS' networking functionality is handled by libp2p, a reusable networking library which was initially developed as part of the IPFS project. Libp2p \citep{libp2p_2023} supports a variety of transport protocols and ensures encrypted communication between peers, thereby promoting security and flexibility.

In the following paragraphs, we describe key components of IPFS’ design concentrating specifically on the those that align with our research objectives. It is worth noting that since IPFS and its subsystems are under continuous development, details of the design may change over time.

\paragraph{Nodes in IPFS}\label{par:nodes_ipfs}
To join the IPFS network, a node initially contacts bootstrap nodes. These bootstrap nodes provide the new node with information about other peers in the network, so that it is able to form its \textit{swarm} or set of known peers. After joining the network, the node generates a unique \hl{peerID} that serves as the node's identifier and is derived from the hash of its public key.

\begin{figure}[htbp]
    \centerline{\includegraphics{figs/Multiaddress.png}}
    \caption{Structure of a Multiaddress. \citep{trautwein_2022}}
    \label{fig:multiaddress}
\end{figure}

Each node maintains part of the global DHT that contains, among others, peer records that map a \hl{peerID} to a set of Multiaddresses at which the peer may be reached \citep{schmahmann_2020}. Multiaddresses \citep{multiaddress_2023} are self-describing, human-readable, and as shown in fig. \ref{fig:multiaddress}, except for the node's \hl{peerID} they encode information about how the peer can be reached in the underlying network, e.g the network address (such as an IPv4, IPv6 address), the transport protocol (such as TCP, UDP, QUIC) and the port number. Basically, Multiaddresses enable nodes to communicate with each other over different network protocols and address types.

\paragraph{Storing content}\label{par:storing_ipfs}
When content is added to IPFS, it is split into chunks, each having a default size of 256 kB (configurable). A unique Content Identifier (CID) is then assigned to each chunk. This CID is created by hashing the chunk's content and appending relevant metadata. As of now, there are two CID versions implemented. The CID v1 format \citep{multiformat}, which will be the default in the near future, consists of four parts

% \[\textsc <cidv1> ::= <multibase><multicodec-version><multicodec-content-type><multihash>\]

\begin{flushleft}
\centering
$\textsc <cidv1> ::= <multibase><multicodec-version><multicodec-content-type><multihash>$
\end{flushleft}

while CID v0 contains only the \(\scriptstyle <multihash>\), which is always a SHA-256 multihash \citep{multiformat}, i.e., a 34-byte hex array with the leading bytes [0x12, 0x20], used to denote the hash function and the hash digest’s size, followed by the hash digest. The rest are implicitly assumed to be (base58btc - cidv0 - dag-pb).

The leading three parts of the CID V1, namely, \(\scriptstyle <multibase>\), \(\scriptstyle <multicodec-version>\), and \(\scriptstyle <multicodec-content-type>\), denote the base in which the CID was encoded, its version, and the type of data it refers to, respectively. Applying base encoding to a CID aims to minimize its overall length. The Multihash represents the hash function that is applied on the binary data along with size of the resulting hash digest. It enables support for multiple hash functions, thus providing flexibility, future-proofing, interoperability and security. % TODO: maybe add citation

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\textwidth]{figs/deduplication.png}}
    \caption{Concept of deduplication. \citep{daniel_2022}}
    \label{fig:deduplication}
\end{figure}

The CIDs of individual chunks are organized in a Merkle Directed Acyclic Graph (DAG) which represents the original content's structure. A Merkle DAG \citep{benet_2014}, is similar to a Merkle tree but without the balance requirement. It allows for a node to have multiple parents, enabling deduplication of chunks (see fig. \ref{fig:deduplication}). This leads to efficient storage and bandwidth usage as the same content doesn't need to be stored or transmitted more than once \citep{trautwein_2022}. The root node of the Merkle DAG is the CID that is used to identify the content.

CIDs are immutable due to their hash-based structure, ensuring that the content cannot be altered without modifying its CID. This enables self-verification, as the CID can be compared with the hash of the content itself for authenticity.

\paragraph{Providing content}\label{par:providing_ipfs}
Revisiting our previous discussion in paragraph \ref{par:nodes_ipfs}, IPFS uses a DHT as a core component of its content routing system. Each node participating in the IPFS network maintains a subset of the DHT that stores three types of key-value pairs, provider records, IPNS records, peer records \citep{schmahmann_2020}. IPNS records are outside the scope of this work and therefore we do not elaborate on them. Peer records are described in paragraph \ref{par:nodes_ipfs}. Provider records map data identifiers to peers that can serve the specific content.

When uploading content, the node stores it locally and then generates a provider record that associates the content's CID with its own \hl{PeerID}. This record is then disseminated into the DHT, signaling to the network that the node can provide the content. The provider record is strategically stored in the DHT segments of the \textit{k} \hl{(\textit{k}= 20)} nodes that are closest to the hash of the content's CID. Closeness, is represented by the XOR distance \citep{maymounkov_2002} of the \hl{node's id} and the CID's hash. By replicating the provider record across \textit{k} nodes, fault-tolerance and availability are enhanced. 

For the upload process to be completed, the node also needs to broadcast its peer record, so that when other nodes wish to retrieve the content, they can learn of its Multiaddresses and establish a connection.

\paragraph{Retrieving content}\label{par:retrieving_ipfs}
% TODO: add citation for bitswap
In IPFS, when a node wants to retrieve content associated with a certain CID, it initially uses the Bitswap protocol to query its direct peers.  If one of these peers has the content, they return it immediately. If unsuccessful, \hl{(after a timeout of one second)} the node leverages the DHT for content discovery.

%TODO: improve the paragraph 
As already mentioned in the previous paragraph , the nodes closest to the requested CID are most likely to hold information about content providers. In this respect, the node forwards a request for the content to \textit{x} \hl{(\textit{x}= 3)} of his peers that are closest to the CID. Based on the circumstances, these nodes either: 1) respond with the requested content (if they have it), 2) return a provider record that points to the peer that hosts the content, 3) suggest other peers closer to the desired CID. The latter is a process that is repeated until either the content or a provider record is obtained, e.g. the node queries the suggested peers, and if they in turn suggest other peers, the node asks those peers, and so forth.

Next, the node, now aware of the \hl{PeerID(s)} hosting the content, queries the DHT again to obtain the corresponding peer records that contain the Multiaddresses of the peers. Note that each node maintains a separate list of peers that it has recently interacted with, which it queries prior to resorting to the DHT, in order to reduce the lookup time.

With the Multiaddresses at hand, the node can now connect to the peers that have the content. Content retrieval then takes place via Bitswap and upon delivery the user can verify the integrity of the received content by reconstructing the CID, ensuring it matches the original request.

\paragraph{Caching}\label{par:caching_ipfs}
Nodes on the IPFS network, have a built-in caching mechanism that allows them to automatically cache content they have recently accessed. When nodes fetch data, they always store a copy in their local cache.

While content is present in a node's local storage, it can be served to other nodes who might request it, reducing retrieval latency, e.g. fetching a cached block from a neighboring node eliminates the overhead of a network traversal. This becomes particularly effective when dealing with popular content that is cached by multiple nodes, as the likelihood that the content is retrieved from nearby sources increases.

Cached content remains available until it is garbage collected. Garbage collection in the field of computer science refers to a process that identifies and removes content that is no longer in use, based on various strategies \citep{gc_2023}. In the case of IPFS, the garbage collection strategy involves identifying and discarding all \emph{``unpinned''} blocks \citep{ipfs_docs_2}. \hl{Explicitly pinned content and files added to the Mutable File System (MFS) - implicitly pinned content - are protected against garbage collection.}

\subsection{Swarm}\label{subsection:swarm}
Swarm is a groundbreaking, peer-to-peer distributed file system developed by the Ethereum Foundation. This technology aims to provide a decentralized and resilient storage layer for the decentralized web. As Tron suggests \citep{tron_2021}, with the Ethereum blockchain as the CPU of the world computer, Swarm is best thought of as its \emph{``hard disk''}. The combination of distributed computing provided by Ethereum and distributed storage offered by Swarm allows for the development of fully decentralized applications. Such DApps benefit from the high availability, zero-downtime, censorship resistance, and integrity offered by Swarm's secure, reliable, and scalable infrastructure.

Swarm utilizes peer-to-peer networking, which is facilitated by libp2p, and distributed hash tables to create a content-addressable network that allows users to upload and retrieve content in a decentralized and fault-tolerant manner. It resembles traditional cloud hosting \citep{swarmwiki_2019}, except that the content is not stored in a central server, but is split into fixed-size chunks, which are then distributed to nodes throughout the network. To achieve this the Swarm team developed an innovative storage model called Distributed Immutable Store for Chunks (DISC) \citep{swarm_team_2021} that aims to provide more efficient data management and retrieval than the traditional DHT.

In order to ensure that nodes are motivated to store and share the data for which they are responsible, Swarm incorporates a built-in incentive system. Even though this aspect guarantees data availability and is crucial for the sustainability of the network in general, it falls outside the scope of our research. Our focus in this work primarily revolves around Swarm's structural design, data distribution, and content retrieval methods.

\paragraph{Content Addressing}\label{par:chunks_swarm}

Chunks are the basic storage units in Swarm and default to a size of 4kB. Currently two types of chunks are supported: content-addressed chunks and single-owner chunks \citep{swarm_team_2021}. We focus on the former.

\begin{figure}[htbp]
    \centerline{\includegraphics[width=\textwidth]{figs/bmt_hash.png}}
    \caption{Hash of chunk in swarm. \citep{daniel_2022}}
    \label{fig:bmt}
\end{figure}

Content-addressed chunks obtain their addresses using the Binary Merkle Tree (BMT) hash function. Basically, as demonstrated in fig. \ref{fig:bmt}, the chunk if needed is zero padded to 4kB and is split in 32 byte segments which are then hashed in pairs using the Keccak256 hash function to build up a Binary Merkle Tree. Finally, the chunk's address is calculated by hashing the Merkle root of the tree with the length of the data (span). Such an approach may impose an overhead due to its increased complexity, compared to applying a hash function directly on the whole chunk, but allows for compact inclusion proofs with a 32-byte resolution. An inclusion proof verifies that a leaf node is a part of a given Merkle tree \citep{wikipedia_2023} and is used as evidence that a storage node possess a chunk.

Swarm's network places a limit of 4 kilobytes on the data size of a content-addressed chunk. The advantage of this small chunk size is that it enables concurrent retrieval even for relatively small files, thereby reducing download latency \citep[p.~38]{tron_2020}. As a result, data larger than 4KB is divided into multiple chunks, which are BMT hashed and organized into a Merkle tree, occupying the leaf nodes. The overall content address of the data, called \textit{Swarm hash}, is the Merkle root and acts as its checksum, enabling the verification of content integrity.

Based on the fact that BMT hash function utilizes the Keccak256 hash function internally, as one might expect, the resulting content's address consists of 32 bytes. However, Swarm also supports encryption of content. In that case the final reference becomes the concatenation of the encrypted content's address and the decryption key, occupying 64 bytes. The size of the references is an important factor especially in the context of DApps since they are usually managed within a blockchain infrastructure.

\paragraph{DISC}\label{par:disc}
The DHTs used in peer-to-peer networks like IPFS and BitTorent, generally, maintain information about which nodes can provide certain content, i.e associate content identifiers with a changing list of seeders \citep{crosby_2007, jimenez_2009}. This information is stored in nodes that are closest to the content's identifier based on some metric. Swarm leverages a more innovative and direct storage model, namely the Distributed Immutable Store for Chunks (DISC). The DISC model shares many traits with the DHT but with a crucial difference: data or more precisely the chunks of a piece of data are stored directly with the closest nodes. The metric of closeness in Swarm is called \textit{proximity order} (PO) and is basically the number of consecutive common most significant bits between two addresses, in this case, the node's address and the chunk's address. 

% TODO: maybe add this
% Both of these addresses are calculated using the Keccak256 hash function which provides uniformity
% Since retrieval in Swarm (2.3.1) assumes that chunks are stored with nodes close to their address, fair and equal load balancing requires that the addresses of chunks should also be uniformly distributed in the address space, and have their content limited and roughly uniform in size.
% As a result of uniformity, a random set of chunked content will generate addresses evenly spread in the address space, i.e. imposing storage
% requirements balanced among nodes.

\begin{figure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figs/dht.pdf}
        \caption{}
        \label{fig:dht}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.5\textwidth}
        \centering
        \raisebox{40pt}{\includegraphics[width=\linewidth]{figs/disc.pdf}} 
        \caption{}
        \label{fig:disc}
    \end{subfigure}
    \caption{Content discovery and retrieval in (a) DHT (b) DISC.}
    \label{fig:content_discovery}
\end{figure}

Fig. \ref{fig:content_discovery} illustrates the process of content discovery and retrieval. The described model can be really effective in large networks since the content is retrieved immediately after locating the closest node, eliminating additional connections and reducing latency. In the simplest case where the closest node is a peer, the retrieval process is reduced to one routing request. On the contrary, a node in IPFS would have to query the DHT twice (see paragraph \ref{par:retrieving_ipfs}) in order to obtain the identity and the addresses of the node that can provide the content and then establish a connection to retrieve it. 

\paragraph{Forwarding \& Backwarding Kademlia}\label{par:forwarding_kademlia}
Most peer-to-peer networks use implementations of the Kademlia protocol that are based on iterative routing for data lookup. Such an implementation is employed by IPFS and is briefly described in paragraph \ref{par:retrieving_ipfs}. However, there are variants of the Kademlia in the literature, for instance R/Kademlia, Pastry and Bamboo \citep{chowdhury_2017},  which use recursive routing to achieve higher performance, reduce latency and bandwith consumption \citep{heep_2010}.

Swarm utilizes a recursive routing protocol which is termed as \textit{forwarding} Kademlia and was \hl{introduced/outlined/described in/ by Tron} \citep{tron_2018}. In this approach, \hl{instead of receiving references back to closer nodes}, the initial request by a node is forwarded by its peers to nodes closer to the data's identifier. Particularly, each node passes the request to a direct peer, i.e. a peer with who they maintain an active connection, that is at least one step closer to the destination in terms of PO. This process continues until the closest node is found.

The DISC (see paragraph \ref{par:disc}) model suggests that content is to be found with nodes closer to its address. In the described routing approach, when a retrieval request reaches its destination (storage node), the response (requested data) is routed back along the same path; the same intermediate nodes pass the delivery back towards the original requestor. This routing method, \hl{for obvious reasons}, is called \textit{backwarding} Kademlia.

Swarm's Kademlia variant facilitates the following important properties:

\begin{itemize}
    \item \textbf{Automatic availability scaling with increased demand}: Nodes involved in fowarding and backwarding chunks, cache them in an opportunistic manner (see paragraph \ref{par:caching_swarm}), thereby increasing availability and facilitating faster access if the same data is requested again. This automatically scales with popular content. 
    \item \textbf{Privacy and Permissionless Interaction}: In the iterative method, the requestor establishes a connection with each node that takes part in the routing process, disclosing their identity. In forwarding Kademlia, the original request appears identical to any request from an intermediate node, thus promoting privacy by protecting the identity of the requestor.
    \item \textbf{Effectiveness (High performance low latency)}: Forwarding Kademlia, in contrast to the iterative method, decreases network traffic by not relying on the online status of known peers. Nodes always forward requests to connected peers. So, there is no need for back-and-forth communication in order to find new seeders when targeted ones leave the network, as in the iterative approach.
\end{itemize}

Overall, Swarm's implementation of forwarding-backwarding Kademlia improves the network's scalability, reduces bandwidth usage and latency, and strengthens user privacy.

\paragraph{Upload and Redundancy}\label{par:upload_swarm}
When data is uploaded to Swarm it is split into chunks, stored in the uploader's local storage and then disseminated into the network via the \textit{push-sync} protocol \citep{swarm_team_2021}. This protocol is responsible for routing a chunk of data to its destination, that is, a storage node whose address is the closest to the chunk's address.

The chunk is forwarded by intermediate nodes along the same route that a retrieval request would take. Once the chunk reaches its destination, a \textit{statement of custody receipt} is sent back along the same path so that uploaders can ensure that their upload is globally retrievable before sharing the content's address \citep[p.~47]{tron_2020}. Just like the retrieval process, all nodes that forward the chunk to its destination do so via identical messages, which leads to anonymous uploading.

However, if the closest node is the only host of a piece of data and it leaves the network, then there is no way to retrieve the content. To ensure redundancy the \textit{pull-sync} protocol was introduced \citep{swarm_team_2021}. This protocol is highly connected to the notion of \textit{responsibility}.

The radius of responsibility \textit{r} of a node is the smallest PO such that the node is connected to all other nodes in the network that have a PO higher than \textit{r}. Nodes in Swarm are responsible for storing and serving chunks whose addresses fall within their radius of responsibility, i.e a chunk's address shares a number \hl{x >= r} of leading bits with the node's address.

The pull-sync protocol aims to sync chunks within a neighborhood of responsibility, as well as maximize resource utilization in the network by pushing chunks to nodes with excess storage. In short, when two nodes connect, they start sharing chunks in both directions. As a result, nodes in a neighborhood eventually replicate all chunks for which they are responsible. Also note that the pull-sync protocol maintains consistency in the neighborhoods as the topology changes (old nodes leave and new ones join).

Therefore, while the push-sync protocol ensures that chunks are initially stored in the right neighborhood, the pull-sync protocol maintains redundancy and consistency in the neighborhood, even as nodes join or leave the network. \sout{For instance, the push sync protocol pushes chunks to nodes in the neighborhood of the chunk's address, and the pull sync protocol ensures that nodes in a neighborhood pull chunks from each other to keep their storages in sync}. The replication of chunks makes the network more resilient as it enhances their availability.

\paragraph{Caching}\label{par:caching_swarm}
Swarm has adopted an opportunistic caching mechanism where forwarding nodes \hl{(nodes that pass chunks to other nodes)} cache chunks they receive and relay in case they are requested again \citep[p.~47]{tron_2020}. More specifically, nodes in Swarm cache:

\begin{itemize}
    \item \textbf{Downloaded chunks}: Nodes cache chunks that they explicitly download.
    \item \textbf{Uploaded chunks}: Nodes cache chunks that they explicitly upload.
    \item \textbf{Requested chunks}: When a retrieval request is made, the desired chunks are forwarded from node to node until they reach the recipient \hl{(requesting node)}. All intermediate nodes cache the chunks.
    \item \textbf{Push-synced chunks}: During the upload process chunks are forwarded until they reach their neighborhood of responsibility. All intermediate nodes cache the chunk.
\end{itemize} 

Nodes in the Swarm network store and serve chunks of data but unlike IPFS, are rewarded for their contribution. They receive a reward every time they serve a chunk. The profitability of a chunk is proportional to its popularity; the more often a chunk is requested, the higher the reward. Accordingly, the garbage collection method used takes into account the profitability factor, which is reasonably determined by the age of the last request (Least Recently Used (LRU) strategy) \citep[p.~72]{tron_2020}. Thus non-popular chunks are more likely to be removed, creating an auto-scaled content distribution network \citep[p.~72]{tron_2020} where popular chunks become more widely spread and much faster to retrieve. This helps optimize the overall performance and efficiency of the network.

Finally, Swarm has also adopted the concept of pinning which protects content against garbage collection, forcing the node to keep it permanently in its local storage. In contrast to IPFS, local pinning is not enough to ensure that a chunk is generally retrievable by other nodes since the pinner might not be in the neighborhood of responsibility where the chunk is meant to be stored. In order to overcome this, global pinning and a recovery process are proposed where pinners re-upload the missing chunk and then serve it to the requester \citep[pp.~161--167]{tron_2020}.

In summary, the automatic caching we described is aided by Swarm's content routing mechanism, which forwards data through the network both during upload and download, and facilitates low latency data retrieval, availability, and robustness against failures. Apparently, popular data which is requested frequently is favored by the caching mechanism. However, even data missing from their neighborhood of responsibility can be retrieved through the recovery process.

\subsection{IPFS \& Swarm Side-by-Side}\label{sec:ipfs_vs_swarm}
% \subsection{Overview of IPFS and Swarm as decentralized storage solutions (IPFS vs Swarm)}\label{sec:}
In this section, we present a side-by-side comparison of IPFS and Swarm emphasizing mainly their differences. By clarifying their distinct architectural and design choices, we aim to offer \hl{comprehensive/broader} overview and a better understanding of how each system achieves data availability, content addressing, censorship resistance, etc.

Content is split into chunks before being uploaded to either of these platforms. A chunk is a piece of data that represents the basic unit of storage and retrieval in IPFS and Swarm. Each chunk is uniquely referenced using its cryptographic hash to achieve content addressing instead of the widely used location addressing. This allows for quick verification of received data and enables deduplication. In Swarm, a Binary Merkle Tree (BMT)  \citep{tron_2020} hash algorithm based on Keccak-256 is used to generate the identifiers of the chunks. Note that for encrypted content, the identifier is the concatenation of the chunk hash and the decryption key and therefore is represented in 64 bytes. The developers of IPFS approached this matter differently to provide more flexibility to end users. They introduced CID  \citep{multiformat}, a self-describing upgradable content-addressed identifier, which consists of the hash digest of the content and some metadata as a prefix. From the metadata, the CID version and its encoding, as well as the hashing function used and the content’s type can be determined.

In order to \emph{``group''} chunks of the same data together, both platforms implement some sort of tree data structure. The root hash of such trees refers to the data as a whole. In IPFS a Merkle Directed Acyclic Graph (DAG)  \citep{benet_2014}, a generalized construction of a Merkle Tree, is used. It allows nodes to have multiple parents, does not need to be balanced, and non-leaf nodes can contain data. Swarm once more embraced the use of Merkle Trees, in which intermediate nodes contain links to leaf nodes that hold the chunks. In the case of uploading folders, IPFS recursively constructs a Merkle DAG, whereas Swarm utilizes a Trie  \citep{tron_2020} data structure. Both these schemes allow for Unix-like path resolving.

Another fundamental difference between IPFS and Swarm lies in what content participating nodes in each network store. In the former, a node holds content depending on its interests, while in the latter a \emph{``cloud''} is formed with every node storing arbitrary chunks of data originating from other nodes \citep{swarmwiki_2019}. In both cases, nodes also cache content they access, until it is \emph{``garbage collected''}. This mechanism is crucial in ensuring that the distribution of popular content scales automatically and retrieval latency is decreased. Swarm takes this one step further by implementing an opportunistic cache model: intermediate nodes cache all chunks they forward.

Moreover, the subjects at hand approach data discoverability and retrieval differently. In IPFS a distributed hash table (DHT) \citep{benet_2014} is used to map CIDs to seeders that can serve that content. In Swarm, a similar model called DISC  \citep{tron_2020} was developed, in which content (value) is directly stored under a corresponding identifier (key), potentially reducing the retrieval process to one routing request. Besides that, in both platforms data are public by default. However, Swarm offers the choice of encrypting data before an upload. Finally, Swarm has a built-in incentive system based on Ethereum, while IPFS depends on a separate blockchain, Filecoin. 

\section{Related work - Purpose and significance of the thesis}\label{sec:related_work}
There are tools for assisting developers to minimize gas consumption, but mostly, the focus is on the program structures rather than the data  \citep{nelaturu_2021, chen_2017, chen_2021}. Among them, we singled out Gasper  \citep{chen_2017}, a cost minimization tool that analyzes bytecode to determine costly patterns in SCs. Its creators used it to analyze approximately 4000 SCs and concluded that three of these patterns were found in the majority of them. This research was continued, and new patterns have been added  \citep{chen_2021}. There is also work on parametric cost bounds in the Ethereum blockchain \citep{albert_2021}.

Data storage cost and gas-saving methods are studied to some extent in the context of specific use cases  \citep{kurt_2020, delgado_2019, westerkamp_2020}. Gas consumption of SCs is examined in  \citep{grech_2020, signer_2018}, but none of them researches the related cost holistically by considering all possible ways of using Ethereum as a data store. In  \citep{consensys}, the use of events as an alternative data storage option was proposed. However, the novel idea of storing data in a transaction’s payload is barely studied. Blockchi \citep{yankov_2018} is a tool that allows storing and retrieving JSON objects, while in  \citep{xie_2017} a storage model is proposed for storing data from IoT sensors in hexadecimal format. In the latter, transaction references are retained in an external database. In our work, apart from evaluating the transaction payload as a data store, we propose an extension to this approach, which we term as \emph{``Unused Function Parameters''}. In short, it is a means of storing data in a transaction's payload while exploiting Solidity's built-in ABI interface to allow handling all available data types.

A common architecture for managing large chunks of data is to use distributed file systems such as IPFS and Swarm for storage and record the resulting content identifiers in Ethereum. This approach has been studied in several applications, e.g., \citep{hao_j_2018, ren_2021}, but no comments were made on the performance of these platforms nor on the cost of recording the content identifiers. Ramesh et al.\cite{ramesh_2019}, exploited such a scheme to handle IoT data and also studied both local retrieval and upload performances of IPFS and Swarm. In \citep{shen_2019}, IPFS was evaluated for remote and local retrieval latency. In \citep{abdullah_2021} the performance of IPFS in a private network was compared to that of FTP. Furthermore, a recent study conducted by Ismail et al. \cite{aisyah_2022} reviewed several works centered around the performance of distributed file systems. In light of the available literature, it is apparent that the majority of attention is being directed towards IPFS, leading us to believe that Swarm’s performance is an under-researched topic. On top of that, it's worth mentioning that research on Swarm's performance prior to three years ago should now be considered outdated, as a new client \citep{swarm_bee} and updates to the underlying protocols were introduced in 2020 \citep{tron_2020}. Our work is focused on local read-write performance of IPFS and Swarm, and the cost associated with storing the corresponding content identifiers in Ethereum.

In conclusion, even though the necessity of minimizing gas consumption is recognized in the literature, existing studies focus mainly on program structures or inefficient patterns. At the same time, efficient data management is an under-researched topic despite the fact that it is an essential part of DApps. In this work, through the research question we posed and the corresponding research we conducted, we intend to give a comprehensive perspective on efficient ways of handling data both on-chain and in hybrid architectures.

\subsection{Existing data management approaches in Ethereum-based dApps}\label{subsection:}
\subsection{Cost and retrieval performance of the aforesaid approaches}\label{subsection:}
\subsection{Cost and retrieval performance of the hybrid approaches}\label{subsection:}
\subsection{Purpose and significance of the thesis}\label{subsection:}

